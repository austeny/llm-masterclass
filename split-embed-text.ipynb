{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and Embedding Text Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open('./fhir-endpoint-for-base-urls.txt') as f:\n",
    "  fhir_endpoint = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=100, # would normally be higher (also max size for chunk)\n",
    "  chunk_overlap=20,\n",
    "  length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 466 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.create_documents([fhir_endpoint])\n",
    "# print(chunks[10].page_content)\n",
    "print(f'Now you have {len(chunks)} chunks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 8048\n",
      "Embedding Cost in USD: 0.003219\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "def print_embedding_cost(texts):\n",
    "  import tiktoken\n",
    "  enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "  total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "  print(f'Total Tokens: {total_tokens}')\n",
    "  print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedding.embed_query('abc')\n",
    "vector = embedding.embed_query(chunks[0].page_content)\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting the Embeddings into a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "pc = pinecone.Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting all indexes\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i in pc.list_indexes().names():\n",
    "  print('deleting all indexes')\n",
    "  pc.delete_index(i)\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index fhir-endpoint-for-base-urls\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'fhir-endpoint-for-base-urls'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "  print(f'Creating index {index_name}')\n",
    "  pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536,\n",
    "    metric='cosine',\n",
    "    spec=pinecone.PodSpec(\n",
    "      environment='gcp-starter'\n",
    "    )\n",
    "  )\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store from document chunks\n",
    "# chunks - split text content with Text Splitter -> Create documents (chunks) from Splitter object -> output is our chunks\n",
    "# embedding - the embeddings class we are using (eg. OpenAIEmbeddings())\n",
    "# index_name - the defined index name we've chosen\n",
    "vector_store = Pinecone.from_documents(chunks, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store from an existing index\n",
    "vector_store = Pinecone.from_existing_index(index_name='fhir-endpoint-for-base-urls', embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking Questions (Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the deadline for this requirement?'\n",
    "result = vector_store.similarity_search(query)\n",
    "# print(result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We proposed that Certified API Developers publish these standardized details by December 31, 2024.\n",
      "--------------------------------------------------\n",
      "90-days would be considered in violation of this proposed requirement.\n",
      "--------------------------------------------------\n",
      "that for the time period between when this final rule is effective and December 31, 2024, that\n",
      "--------------------------------------------------\n",
      "Rule to provide industry an opportunity to coalesce on specifications. We finalized § 170.404(b)(2)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "  print(r.page_content)\n",
    "  print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "  search_type='similarity',\n",
    "  search_kwargs={\n",
    "    'k': 3\n",
    "  }\n",
    " )\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there is no specific mention of an address being required in the Organization resource. However, the organization details that should be used by app developers include endpoints. If address information is not specified as a required element in the Organization resource in your specific context, then it may not be necessary to add it. It would be best to refer to the complete documentation for clear guidance on which elements are required for the Organization resource.\n"
     ]
    }
   ],
   "source": [
    "# query = 'What is the deadline for this requirement?'\n",
    "# query = 'What FHIR Resources are used in this rule?'\n",
    "# query = 'Give me a detailed explanation of what needs to be implemented as a requirement for Healthcare API Developers in this rule?'\n",
    "query = 'Do I need to add address to Organization resource'\n",
    "answer = chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
